{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2374 entries, 8122 to 11295\n",
      "Data columns (total 6 columns):\n",
      "id                2374 non-null int64\n",
      "author            2374 non-null object\n",
      "perspective_id    2374 non-null int64\n",
      "evidence_id       2374 non-null int64\n",
      "comment           2374 non-null object\n",
      "anno              2374 non-null object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 129.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_path = \"../data/pilot14_step3_pilot/webapp_evidencerelation.csv\"\n",
    "\n",
    "_df = pd.read_csv(result_path)\n",
    "df = _df[(_df.comment == 'pilot') & (_df.author != 'Sihao') & (_df.author != 'Daniel')]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate turker results\n",
    "\n",
    "arr_al = []\n",
    "_data = []\n",
    "\n",
    "evidences = df.evidence_id.unique()\n",
    "\n",
    "for eid in evidences:\n",
    "    e_df = df[df.evidence_id == eid]\n",
    "    \n",
    "    persps = e_df.perspective_id.unique()\n",
    "    \n",
    "    for pid in persps:\n",
    "        p_df = e_df[e_df.perspective_id == pid]\n",
    "        \n",
    "        group = p_df.anno.value_counts()\n",
    "        \n",
    "        _s = _n = 0\n",
    "        if \"S\" in group.index:\n",
    "            _s = group.S\n",
    "        if \"N\" in group.index:\n",
    "            _n = group.N\n",
    "        \n",
    "        _data.append([eid, pid, _s, _n])\n",
    "\n",
    "\n",
    "result_path = \"../data/pilot14_step3_pilot/annotation_counts.csv\"\n",
    "with open(result_path, 'w') as fout:\n",
    "    fout.write(\"evidence,perspective,sup,nsup\\n\")\n",
    "    for entry in _data:\n",
    "        _entry = [str(x) for x in entry]\n",
    "        fout.write(\",\".join(_entry)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 0 to 399\n",
      "Data columns (total 4 columns):\n",
      "perspective_id    400 non-null int64\n",
      "evidence_id       400 non-null int64\n",
      "sihao_label       400 non-null object\n",
      "daniel_label      400 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 15.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Merge annotations from Sihao and Daniel\n",
    "\n",
    "sihao_df = _df[_df.author == 'Sihao']\n",
    "daniel_df = _df[_df.author == 'Daniel']\n",
    "\n",
    "# Rename the annotation columns to specify the author\n",
    "sihao_df = sihao_df[[\"perspective_id\", \"evidence_id\", \"anno\"]]\n",
    "daniel_df = daniel_df[[\"perspective_id\", \"evidence_id\", \"anno\"]]\n",
    "sihao_df = sihao_df.rename(index=str, columns={\"anno\": \"sihao_label\"})\n",
    "daniel_df = daniel_df.rename(index=str, columns={\"anno\": \"daniel_label\"})\n",
    "\n",
    "merged_df = pd.merge(sihao_df, daniel_df, on=['evidence_id', 'perspective_id'])\n",
    "merged_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 0 to 399\n",
      "Data columns (total 6 columns):\n",
      "perspective_id    400 non-null int64\n",
      "evidence_id       400 non-null int64\n",
      "sihao_label       400 non-null object\n",
      "daniel_label      400 non-null object\n",
      "evidence          400 non-null object\n",
      "perspective       400 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 41.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Add perspective and evidence content for adjudication\n",
    "\n",
    "perspective = \"/home/squirrel/ccg-new/projects/perspective/data/database_output/re-step1/webapp_perspective.csv\"\n",
    "evidence = \"/home/squirrel/ccg-new/projects/perspective/data/database_output/re-step1/webapp_evidence.csv\"\n",
    "\n",
    "merged_df[\"evidence\"] = \"\"\n",
    "merged_df[\"perspective\"] = \"\"\n",
    "\n",
    "pdf = pd.read_csv(perspective)\n",
    "edf = pd.read_csv(evidence)\n",
    "\n",
    "for idx, row in merged_df.iterrows():\n",
    "    pid = row.perspective_id\n",
    "    eid = row.evidence_id\n",
    "    merged_df.at[idx, \"evidence\"] = edf[edf.id==eid].iloc[0].content\n",
    "    merged_df.at[idx, \"perspective\"] = pdf[pdf.id==pid].iloc[0].title\n",
    "    \n",
    "merged_df.info()\n",
    "\n",
    "result_path = \"../data/pilot14_step3_pilot/our_annotations.csv\"\n",
    "\n",
    "merged_df.to_csv(result_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404 entries, 0 to 403\n",
      "Data columns (total 3 columns):\n",
      "perspective_id    400 non-null float64\n",
      "evidence_id       400 non-null float64\n",
      "Adjudicated       400 non-null object\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 9.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 720 entries, 0 to 719\n",
      "Data columns (total 5 columns):\n",
      "evidence       720 non-null int64\n",
      "perspective    720 non-null int64\n",
      "sup            720 non-null int64\n",
      "nsup           720 non-null int64\n",
      "p_i            720 non-null float64\n",
      "dtypes: float64(1), int64(4)\n",
      "memory usage: 28.2 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 0 to 399\n",
      "Data columns (total 8 columns):\n",
      "evidence          400 non-null int64\n",
      "perspective       400 non-null int64\n",
      "sup               400 non-null int64\n",
      "nsup              400 non-null int64\n",
      "p_i               400 non-null float64\n",
      "perspective_id    400 non-null float64\n",
      "evidence_id       400 non-null float64\n",
      "Adjudicated       400 non-null object\n",
      "dtypes: float64(3), int64(4), object(1)\n",
      "memory usage: 28.1+ KB\n"
     ]
    }
   ],
   "source": [
    "adjudcation = \"../data/pilot14_step3_pilot/adjudication.csv\"\n",
    "agreement = \"../data/pilot14_step3_pilot/agreement_enhanced.csv\"\n",
    "\n",
    "adj_df = pd.read_csv(ajudcation)[[\"perspective_id\", \"evidence_id\", \"Adjudicated\"]]\n",
    "adj_df.info()\n",
    "\n",
    "agr_df = pd.read_csv(agreement)\n",
    "agr_df.info()\n",
    "\n",
    "adj_agr_df = pd.merge(agr_df, adj_df, left_on=['evidence', 'perspective'], right_on=['evidence_id', 'perspective_id'])\n",
    "adj_agr_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (All perspectives) = 0.765\n",
      "Count = 400 out of 400\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy of ALL evidences/perspectivs with respect of our adjudicated annotations\n",
    "\n",
    "correct = 0\n",
    "for idx, row in adj_agr_df.iterrows():\n",
    "    if row.sup > row.nsup:\n",
    "        label = \"S\"\n",
    "    elif row.sup == row.nsup:\n",
    "        label = \"D\"\n",
    "    else:\n",
    "        label = \"N\"\n",
    "    \n",
    "    if label == row.Adjudicated:\n",
    "        correct += 1\n",
    "\n",
    "print('Accuracy (All perspectives) = {}'.format(correct/len(adj_agr_df.index)))\n",
    "print('Count = {} out of {}'.format(len(adj_agr_df.index), len(adj_agr_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (p_i = 1 only) = 0.9271523178807947\n",
      "Count = 151 out of 400\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy of ABSOLUTE AGREEMENT (p_i = 1) evidences/perspectivs with respect of our adjudicated annotations\n",
    "correct = 0\n",
    "\n",
    "hq_df = adj_agr_df[adj_agr_df[\"p_i\"] == 1]\n",
    "\n",
    "for idx, row in hq_df.iterrows():\n",
    "    if row.sup > row.nsup:\n",
    "        label = \"S\"\n",
    "    elif row.sup == row.nsup:\n",
    "        label = \"D\"\n",
    "    else:\n",
    "        label = \"N\"\n",
    "    \n",
    "    if label == row.Adjudicated:\n",
    "        correct += 1\n",
    "\n",
    "print('Accuracy (p_i = 1 only) = {}'.format(correct/len(hq_df.index)))\n",
    "print('Count = {} out of {}'.format(len(hq_df.index), len(adj_agr_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (p_i >= 0.5) = 0.8992537313432836\n",
      "Count = 268 out of 400\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy of HIGH AGREEMENT (p_i >= 0.5) evidences/perspectivs with respect of our adjudicated annotations\n",
    "correct = 0\n",
    "\n",
    "hq_df = adj_agr_df[adj_agr_df[\"p_i\"] >= 0.5]\n",
    "\n",
    "for idx, row in hq_df.iterrows():\n",
    "    if row.sup > row.nsup:\n",
    "        label = \"S\"\n",
    "    elif row.sup == row.nsup:\n",
    "        label = \"D\"\n",
    "    else:\n",
    "        label = \"N\"\n",
    "    \n",
    "    if label == row.Adjudicated:\n",
    "        correct += 1\n",
    "\n",
    "print('Accuracy (p_i >= 0.5) = {}'.format(correct/len(hq_df.index)))\n",
    "print('Count = {} out of {}'.format(len(hq_df.index), len(adj_agr_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 720 entries, 0 to 719\n",
      "Data columns (total 6 columns):\n",
      "evidence       720 non-null int64\n",
      "perspective    720 non-null int64\n",
      "sup            720 non-null int64\n",
      "nsup           720 non-null int64\n",
      "p_i            720 non-null float64\n",
      "label          720 non-null object\n",
      "dtypes: float64(1), int64(4), object(1)\n",
      "memory usage: 33.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Prepare annotations\n",
    "agr_df[\"label\"] = \"\"\n",
    "\n",
    "for idx, row in agr_df.iterrows():\n",
    "    vote_sup = row.sup\n",
    "    vote_nsup = row.nsup\n",
    "    \n",
    "    if vote_sup > vote_nsup:\n",
    "        agr_df.at[idx, \"label\"] = \"S\"\n",
    "    elif vote_sup == vote_nsup:\n",
    "        agr_df.at[idx, \"label\"] = \"D\"\n",
    "    else:\n",
    "        agr_df.at[idx, \"label\"] = \"N\"\n",
    "        \n",
    "agr_df.info()\n",
    "\n",
    "out_path = \"../data/pilot14_step3_pilot/agreement_w_label.csv\"\n",
    "agr_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
