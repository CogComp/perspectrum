{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 258875 entries, 11296 to 270170\n",
      "Data columns (total 6 columns):\n",
      "id                258875 non-null int64\n",
      "author            258875 non-null object\n",
      "perspective_id    258875 non-null int64\n",
      "evidence_id       258875 non-null int64\n",
      "comment           258875 non-null object\n",
      "anno              258875 non-null object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 13.8+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_path = \"../data/pilot14_step3_pilot/webapp_evidencerelation.csv\"\n",
    "\n",
    "_df = pd.read_csv(result_path)\n",
    "df = _df[(_df.comment == 'step3') & (_df.author != 'GOLD')]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate turker results\n",
    "\n",
    "arr_al = []\n",
    "_data = []\n",
    "\n",
    "evidences = df.evidence_id.unique()\n",
    "\n",
    "for eid in evidences:\n",
    "    e_df = df[df.evidence_id == eid]\n",
    "    \n",
    "    persps = e_df.perspective_id.unique()\n",
    "    \n",
    "    for pid in persps:\n",
    "        p_df = e_df[e_df.perspective_id == pid]\n",
    "        \n",
    "        group = p_df.anno.value_counts()\n",
    "        \n",
    "        _s = _n = 0\n",
    "        if \"S\" in group.index:\n",
    "            _s = group.S\n",
    "        if \"N\" in group.index:\n",
    "            _n = group.N\n",
    "        \n",
    "        _data.append([eid, pid, _s, _n])\n",
    "\n",
    "\n",
    "result_path = \"../data/pilot14_step3_pilot/annotation_counts.csv\"\n",
    "with open(result_path, 'w') as fout:\n",
    "    fout.write(\"evidence,perspective,sup,nsup\\n\")\n",
    "    for entry in _data:\n",
    "        _entry = [str(x) for x in entry]\n",
    "        fout.write(\",\".join(_entry)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 0 to 399\n",
      "Data columns (total 4 columns):\n",
      "perspective_id    400 non-null int64\n",
      "evidence_id       400 non-null int64\n",
      "sihao_label       400 non-null object\n",
      "daniel_label      400 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 15.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Merge annotations from Sihao and Daniel\n",
    "\n",
    "sihao_df = _df[_df.author == 'Sihao']\n",
    "daniel_df = _df[_df.author == 'Daniel']\n",
    "\n",
    "# Rename the annotation columns to specify the author\n",
    "sihao_df = sihao_df[[\"perspective_id\", \"evidence_id\", \"anno\"]]\n",
    "daniel_df = daniel_df[[\"perspective_id\", \"evidence_id\", \"anno\"]]\n",
    "sihao_df = sihao_df.rename(index=str, columns={\"anno\": \"sihao_label\"})\n",
    "daniel_df = daniel_df.rename(index=str, columns={\"anno\": \"daniel_label\"})\n",
    "\n",
    "merged_df = pd.merge(sihao_df, daniel_df, on=['evidence_id', 'perspective_id'])\n",
    "merged_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 0 to 399\n",
      "Data columns (total 6 columns):\n",
      "perspective_id    400 non-null int64\n",
      "evidence_id       400 non-null int64\n",
      "sihao_label       400 non-null object\n",
      "daniel_label      400 non-null object\n",
      "evidence          400 non-null object\n",
      "perspective       400 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 41.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Add perspective and evidence content for adjudication\n",
    "\n",
    "perspective = \"/home/squirrel/ccg-new/projects/perspective/data/database_output/re-step1/webapp_perspective.csv\"\n",
    "evidence = \"/home/squirrel/ccg-new/projects/perspective/data/database_output/re-step1/webapp_evidence.csv\"\n",
    "\n",
    "merged_df[\"evidence\"] = \"\"\n",
    "merged_df[\"perspective\"] = \"\"\n",
    "\n",
    "pdf = pd.read_csv(perspective)\n",
    "edf = pd.read_csv(evidence)\n",
    "\n",
    "for idx, row in merged_df.iterrows():\n",
    "    pid = row.perspective_id\n",
    "    eid = row.evidence_id\n",
    "    merged_df.at[idx, \"evidence\"] = edf[edf.id==eid].iloc[0].content\n",
    "    merged_df.at[idx, \"perspective\"] = pdf[pdf.id==pid].iloc[0].title\n",
    "    \n",
    "merged_df.info()\n",
    "\n",
    "result_path = \"../data/pilot14_step3_pilot/our_annotations.csv\"\n",
    "\n",
    "merged_df.to_csv(result_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404 entries, 0 to 403\n",
      "Data columns (total 3 columns):\n",
      "perspective_id    400 non-null float64\n",
      "evidence_id       400 non-null float64\n",
      "Adjudicated       400 non-null object\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 9.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 720 entries, 0 to 719\n",
      "Data columns (total 5 columns):\n",
      "evidence       720 non-null int64\n",
      "perspective    720 non-null int64\n",
      "sup            720 non-null int64\n",
      "nsup           720 non-null int64\n",
      "p_i            720 non-null float64\n",
      "dtypes: float64(1), int64(4)\n",
      "memory usage: 28.2 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 0 to 399\n",
      "Data columns (total 8 columns):\n",
      "evidence          400 non-null int64\n",
      "perspective       400 non-null int64\n",
      "sup               400 non-null int64\n",
      "nsup              400 non-null int64\n",
      "p_i               400 non-null float64\n",
      "perspective_id    400 non-null float64\n",
      "evidence_id       400 non-null float64\n",
      "Adjudicated       400 non-null object\n",
      "dtypes: float64(3), int64(4), object(1)\n",
      "memory usage: 28.1+ KB\n"
     ]
    }
   ],
   "source": [
    "adjudcation = \"../data/pilot14_step3_pilot/pilot/adjudication.csv\"\n",
    "agreement = \"../data/pilot14_step3_pilot/pilot/agreement_enhanced.csv\"\n",
    "\n",
    "adj_df = pd.read_csv(adjudcation)[[\"perspective_id\", \"evidence_id\", \"Adjudicated\"]]\n",
    "adj_df.info()\n",
    "\n",
    "agr_df = pd.read_csv(agreement)\n",
    "agr_df.info()\n",
    "\n",
    "adj_agr_df = pd.merge(agr_df, adj_df, left_on=['evidence', 'perspective'], right_on=['evidence_id', 'perspective_id'])\n",
    "adj_agr_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (All perspectives) = 0.8775\n",
      "Count = 400 out of 400\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy of ALL evidences/perspectivs with respect of our adjudicated annotations\n",
    "# if for a given evidence and perspective relation annotation, p_i < 0.5, we take it as \"not related\", as we will do so in our dataset constrcution \n",
    "\n",
    "correct = 0\n",
    "for idx, row in adj_agr_df.iterrows():\n",
    "    if row.p_i < 0.5:\n",
    "        label = \"N\"\n",
    "    elif row.sup > row.nsup:\n",
    "        label = \"S\"\n",
    "    else:\n",
    "        label = \"N\"\n",
    "    \n",
    "    if label == row.Adjudicated:\n",
    "        correct += 1\n",
    "\n",
    "print('Accuracy (All perspectives) = {}'.format(correct/len(adj_agr_df.index)))\n",
    "print('Count = {} out of {}'.format(len(adj_agr_df.index), len(adj_agr_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (p_i = 1 only) = 0.9271523178807947\n",
      "Count = 151 out of 400\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy of ABSOLUTE AGREEMENT (p_i = 1) evidences/perspectivs with respect of our adjudicated annotations\n",
    "correct = 0\n",
    "\n",
    "hq_df = adj_agr_df[adj_agr_df[\"p_i\"] == 1]\n",
    "\n",
    "for idx, row in hq_df.iterrows():\n",
    "    if row.sup > row.nsup:\n",
    "        label = \"S\"\n",
    "    elif row.sup == row.nsup:\n",
    "        label = \"D\"\n",
    "    else:\n",
    "        label = \"N\"\n",
    "    \n",
    "    if label == row.Adjudicated:\n",
    "        correct += 1\n",
    "\n",
    "print('Accuracy (p_i = 1 only) = {}'.format(correct/len(hq_df.index)))\n",
    "print('Count = {} out of {}'.format(len(hq_df.index), len(adj_agr_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (p_i >= 0.5) = 0.8992537313432836\n",
      "Count = 268 out of 400\n"
     ]
    }
   ],
   "source": [
    "# Measure accuracy of HIGH AGREEMENT (p_i >= 0.5) evidences/perspectivs with respect of our adjudicated annotations\n",
    "correct = 0\n",
    "\n",
    "hq_df = adj_agr_df[adj_agr_df[\"p_i\"] >= 0.5]\n",
    "\n",
    "for idx, row in hq_df.iterrows():\n",
    "    if row.sup > row.nsup:\n",
    "        label = \"S\"\n",
    "    elif row.sup == row.nsup:\n",
    "        label = \"D\"\n",
    "    else:\n",
    "        label = \"N\"\n",
    "    \n",
    "    if label == row.Adjudicated:\n",
    "        correct += 1\n",
    "\n",
    "print('Accuracy (p_i >= 0.5) = {}'.format(correct/len(hq_df.index)))\n",
    "print('Count = {} out of {}'.format(len(hq_df.index), len(adj_agr_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 720 entries, 0 to 719\n",
      "Data columns (total 6 columns):\n",
      "evidence       720 non-null int64\n",
      "perspective    720 non-null int64\n",
      "sup            720 non-null int64\n",
      "nsup           720 non-null int64\n",
      "p_i            720 non-null float64\n",
      "label          720 non-null object\n",
      "dtypes: float64(1), int64(4), object(1)\n",
      "memory usage: 33.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Prepare annotations\n",
    "agr_df[\"label\"] = \"\"\n",
    "\n",
    "for idx, row in agr_df.iterrows():\n",
    "    vote_sup = row.sup\n",
    "    vote_nsup = row.nsup\n",
    "    \n",
    "    if vote_sup > vote_nsup:\n",
    "        agr_df.at[idx, \"label\"] = \"S\"\n",
    "    elif vote_sup == vote_nsup:\n",
    "        agr_df.at[idx, \"label\"] = \"D\"\n",
    "    else:\n",
    "        agr_df.at[idx, \"label\"] = \"N\"\n",
    "        \n",
    "agr_df.info()\n",
    "\n",
    "out_path = \"../data/pilot14_step3_pilot/agreement_w_label.csv\"\n",
    "agr_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62989 entries, 0 to 62988\n",
      "Data columns (total 6 columns):\n",
      "evidence       62989 non-null int64\n",
      "perspective    62989 non-null int64\n",
      "sup            62989 non-null int64\n",
      "nsup           62989 non-null int64\n",
      "p_i            62989 non-null float64\n",
      "label          62989 non-null object\n",
      "dtypes: float64(1), int64(4), object(1)\n",
      "memory usage: 2.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8122 entries, 0 to 8121\n",
      "Data columns (total 6 columns):\n",
      "id                8122 non-null int64\n",
      "author            8122 non-null object\n",
      "perspective_id    8122 non-null int64\n",
      "evidence_id       8122 non-null int64\n",
      "comment           0 non-null float64\n",
      "anno              8122 non-null object\n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 380.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Calculate how many annotations\n",
    "path = \"../data/pilot14_step3_pilot/annotation_counts_label.csv\"\n",
    "gold_path = \"../data/database_output/re-step1/webapp_evidencerelation.csv\"\n",
    "\n",
    "result = pd.read_csv(path)\n",
    "gold = pd.read_csv(gold_path)\n",
    "\n",
    "result.info()\n",
    "gold.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1621\n",
      "Recall (number of evidences in gold verified by AMT annotation): 3586/4018 = 0.8924838227974117\n",
      "Number of evidences retained: 5297/8122 = 0.6521792661905934\n"
     ]
    }
   ],
   "source": [
    "pred_persp = set(result.perspective.unique())\n",
    "gold_presp = set(gold.perspective_id.unique())\n",
    "\n",
    "\n",
    "\n",
    "# Valid evidences annotations in the original gold set\n",
    "all_persps = [p for p in pred_persp if p in gold_presp]\n",
    "\n",
    "pred_sup = result[result.label == \"S\"]\n",
    "\n",
    "pred_sup_perspective = pred_sup.perspective.unique()\n",
    "\n",
    "# Number of correctly annotated \"S\" in gold annotations\n",
    "correct = [p for p in pred_sup_perspective if p in all_persps]\n",
    "\n",
    "# Number of evidences\n",
    "pred_evi_num = set(pred_sup.evidence.unique())\n",
    "\n",
    "# Compute how many evidences not connected to anything and also have low agreement\n",
    "low_agree_df = result[result.p_i < 0.5]\n",
    "low_agree_evi_ids = set(low_agree_df.evidence.unique())\n",
    "all_evi_ids = set(gold.id.unique())\n",
    "not_connected_evi_ids = all_evi_ids - pred_evi_num\n",
    "confusing_evi_ids = low_agree_evi_ids.intersection(not_connected_evi_ids)\n",
    "\n",
    "print(len(confusing_evi_ids))\n",
    "print(\"Recall (number of evidences in gold verified by AMT annotation): {}/{} = {}\".format(len(correct), len(all_persps), len(correct)/len(all_persps)))\n",
    "print(\"Number of evidences retained: {}/{} = {}\".format(len(pred_evi_num), len(gold.index), len(pred_evi_num)/len(gold.index)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
