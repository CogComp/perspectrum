{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "human_test_set = [105,888,673,177,818,183,700,592,682,622,993,922,936,218,360,826,512,131,897,743]\n",
    "\n",
    "persp_data_path = \"../data/dataset/perspective_pool_v0.2.json\"\n",
    "claim_data_path = \"../data/dataset/perspectrum_with_answers_v0.2.json\"\n",
    "\n",
    "persps = json.load(open(persp_data_path))\n",
    "claims = json.load(open(claim_data_path))\n",
    "\n",
    "persp_dict = {}\n",
    "claim_dict = {}\n",
    "for p in persps:\n",
    "    persp_dict[p[\"pId\"]] = p[\"text\"]\n",
    "\n",
    "for c in claims:\n",
    "    claim_dict[c[\"cId\"]] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make gold annotation\n",
    "data = []\n",
    "\n",
    "for cid in human_test_set:\n",
    "    cur_c = claim_dict[cid]\n",
    "    c_text = cur_c['text']\n",
    "    \n",
    "    p_clusters = [(p['pids'], p['stance_label_3']) for p in cur_c['perspectives']]\n",
    "    \n",
    "    p_title_clusters = []\n",
    "    for c, stance in p_clusters:\n",
    "        c = [(pid, persp_dict[pid]) for pid in c]\n",
    "        p_title_clusters.append((c, stance)) \n",
    "        \n",
    "    data.append({\n",
    "        \"claim_id\": cid,\n",
    "        \"claim_text\": c_text,\n",
    "        \"gold_perspectives\" : p_title_clusters\n",
    "    })\n",
    "    \n",
    "out_path = \"../data/dataset/human_eval/equivalence_human_eval_gold.json\"\n",
    "json.dump(data, open(out_path, 'w'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the table for stance annotation\n",
    "import csv\n",
    "\n",
    "out_path = \"../data/dataset/human_eval/stance_human_eval.csv\"\n",
    "\n",
    "f = open(out_path, 'w')\n",
    "fieldnames = ['claim_id', 'claim_title', 'perspective_id', 'perpsective_title', 'gold_stance']\n",
    "\n",
    "writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "writer.writeheader()\n",
    "\n",
    "for c in data:\n",
    "    \n",
    "    for clusters in c[\"gold_perspectives\"]:\n",
    "        stance = clusters[1]\n",
    "        for p, title in clusters[0]:\n",
    "            writer.writerow({\n",
    "                \"claim_id\": c[\"claim_id\"],\n",
    "                \"claim_title\": c[\"claim_text\"],\n",
    "                \"perspective_id\": p,\n",
    "                \"perpsective_title\": title,\n",
    "                \"gold_stance\": stance\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make spreadsheet for equivalence\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "out_path = \"../data/dataset/human_eval/equivalence_human_eval.csv\"\n",
    "\n",
    "f = open(out_path, 'w')\n",
    "fieldnames = ['claim_id', 'claim_title', 'perspective_id_1', 'perpsective_title_1', 'perspective_id_2', 'perpsective_title_2']\n",
    "\n",
    "writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "writer.writeheader()\n",
    "\n",
    "for c in data:\n",
    "    all_p = []\n",
    "\n",
    "    for clusters in c[\"gold_perspectives\"]:\n",
    "        for p, title in clusters[0]:\n",
    "            all_p.append((p, title))\n",
    "    \n",
    "    l_p = len(all_p)\n",
    "    \n",
    "    cartesian = list(itertools.combinations(all_p,2))\n",
    "        \n",
    "    for p1, p2 in cartesian:\n",
    "                \n",
    "        writer.writerow({\n",
    "            \"claim_id\": c[\"claim_id\"],\n",
    "            \"claim_title\": c[\"claim_text\"],\n",
    "            \"perspective_id_1\": p1[0],\n",
    "            \"perpsective_title_1\": p1[1],\n",
    "            \"perspective_id_2\": p2[0],\n",
    "            \"perpsective_title_2\": p2[1],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/squirrel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8695652173913043, 0.8333333333333334, 0.851063829787234)\n",
      "(0.8809523809523809, 0.7708333333333334, 0.8222222222222222)\n"
     ]
    }
   ],
   "source": [
    "# Let's see how our two annotators, Rick and Daniel did!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def is_equivalent_in_gold(pid1, pid2, gold_clusters):\n",
    "    result = False\n",
    "    for c in gold_clusters:\n",
    "        if (pid1 in c) and (pid2 in c):\n",
    "            result = True\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def compute_p_r_f1(df):\n",
    "        \n",
    "    tp = fp = tn = fn = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        cid = row.claim_id\n",
    "        pid1 = row.perspective_id_1\n",
    "        pid2 = row.perspective_id_2\n",
    "        \n",
    "        \n",
    "        pred_eq = not df.isnull().ix[idx,'Equivalent']\n",
    "        gold_eq = is_equivalent_in_gold(pid1, pid2, gold_cluster_dict[cid])\n",
    "        \n",
    "        if pred_eq and gold_eq:\n",
    "            tp += 1\n",
    "        elif pred_eq and not gold_eq:\n",
    "            fp += 1\n",
    "        elif not pred_eq and gold_eq:\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "            \n",
    "    \n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "\n",
    "    f1 = 2 * prec * rec / (prec + rec)\n",
    "    return (prec, rec, f1)\n",
    "\n",
    "rick_result_path = '../data/dataset/human_eval/equivalence_rick.csv'\n",
    "daniel_result_path = '../data/dataset/human_eval/equivalence_daniel.csv'\n",
    "gold_path = '../data/dataset/human_eval/equivalence_human_eval_gold.json'\n",
    "\n",
    "rdf = pd.read_csv(rick_result_path)\n",
    "ddf = pd.read_csv(daniel_result_path)\n",
    "\n",
    "gold_data = json.load(open(gold_path))\n",
    "gold_cluster_dict = {}\n",
    "for data in gold_data:\n",
    "    clusters = []\n",
    "    for c in data[\"gold_perspectives\"]:\n",
    "        clusters.append([p[0] for p in c[0]])\n",
    "        \n",
    "    gold_cluster_dict[data['claim_id']] = clusters\n",
    "    \n",
    "# # We only got to row 718...\n",
    "rdf = rdf[rdf.index < 718]\n",
    "ddf = ddf[ddf.index < 718]\n",
    "\n",
    "\n",
    "print(compute_p_r_f1(rdf))\n",
    "print(compute_p_r_f1(ddf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[777], [781]]\n",
      "[[7158], [7157], [7153], [7156]]\n",
      "[[5604]]\n",
      "[[1314, 22061, 22062], [5825, 26012, 26013]]\n",
      "[[6566], [6570], [6564, 6565]]\n",
      "[[1363, 22149]]\n",
      "[[5817, 26004, 26005], [2115, 5812], [5810], [5814], [2113, 5809, 22858, 22857], [5816, 26000, 26003], [5818, 26006, 26007], [5819, 26008, 26009], [5813], [25996, 25997, 25998], [2119, 22863, 22864], [2120], [2114, 22859, 22860], [2116, 22861, 22862], [2117]]\n",
      "[[4931], [4929], [4924]]\n",
      "[[5151], [5157, 25598, 25599], [5154, 25596, 25597], [5150], [5152], [5148], [5153, 25592, 25593, 25594, 25595], [5156], [5149]]\n",
      "Precision = 0.6382716049382716\n",
      "Recall = 0.8379629629629629\n",
      "F1 = 0.7246110840089514\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the result on perspective relevance\n",
    "import pandas as pd\n",
    "import json\n",
    "import statistics\n",
    "\n",
    "persp_data_path = \"../data/dataset/perspective_pool_v0.2.json\"\n",
    "claim_data_path = \"../data/dataset/perspectrum_with_answers_v0.2.json\"\n",
    "\n",
    "persps = json.load(open(persp_data_path))\n",
    "claims = json.load(open(claim_data_path))\n",
    "\n",
    "persp_dict = {}\n",
    "claim_dict = {}\n",
    "\n",
    "for p in persps:\n",
    "    persp_dict[p[\"pId\"]] = p[\"text\"]\n",
    "\n",
    "for c in claims:\n",
    "    claim_dict[c[\"cId\"]] = c\n",
    "    \n",
    "human_anno_table = \"../data/dataset/human_eval/webapp_humanannotation.csv\"\n",
    "\n",
    "df = pd.read_csv(human_anno_table)\n",
    "\n",
    "sihao_df = df[df.author == 'Sihao']\n",
    "\n",
    "prec_list = []\n",
    "rec_list = []\n",
    "\n",
    "for idx, row in sihao_df.iterrows():\n",
    "    cid = row.claim_id\n",
    "    annotations = json.loads(row.annotation)\n",
    "    gold_c = claim_dict[cid]\n",
    "    clusters = [_c[\"pids\"] for _c in gold_c[\"perspectives\"]]\n",
    "    covered = [False for _ in clusters]\n",
    "    for anno in annotations:\n",
    "        pred_pid = anno[\"pId\"]\n",
    "        \n",
    "        for idx, _c in enumerate(clusters):\n",
    "            if pred_pid in _c:\n",
    "                covered[idx] = True\n",
    "                \n",
    "                \n",
    "    \n",
    "    tp = [_co for _co in covered if _co]\n",
    "    prec = len(tp) / len(covered)\n",
    "    \n",
    "    if len(annotations) == 0:\n",
    "        rec = 1\n",
    "    else:\n",
    "        rec = len(tp) / len(annotations)\n",
    "    \n",
    "    prec_list.append(prec)\n",
    "    rec_list.append(rec)\n",
    "    \n",
    "mean_prec = statistics.mean(prec_list)\n",
    "mean_rec = statistics.mean(rec_list)\n",
    "mean_f1 = 2 * mean_rec * mean_prec / (mean_prec + mean_rec)\n",
    "print(\"Precision = {}\".format(mean_prec))\n",
    "print(\"Recall = {}\".format(mean_rec))\n",
    "print(\"F1 = {}\".format(mean_f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1, 0.42857142857142855, 0.0, 0.14285714285714285] [1.0, 1.0, 1, 1.0, 1.0, 1, 0.5, 1, 0.5]\n",
      "Precision = 0.544973544973545\n",
      "Recall = 0.8888888888888888\n",
      "F1 = 0.6756867568675686\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the result on perspective substantiation\n",
    "prec_list = []\n",
    "rec_list = []\n",
    "\n",
    "for idx, row in sihao_df.iterrows():\n",
    "    cid = row.claim_id\n",
    "    annotations = json.loads(row.annotation)\n",
    "    gold_c = claim_dict[cid]\n",
    "    clusters = gold_c[\"perspectives\"]\n",
    "    covered = [False for _ in clusters]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    tp = fp = fn = 0\n",
    "    for anno in annotations:\n",
    "        pred_pid = anno[\"pId\"]\n",
    "        pred_evidences = json.loads(anno[\"evidences\"])\n",
    "        \n",
    "        for _c in clusters:\n",
    "            if pred_pid in _c[\"pids\"]:\n",
    "                gold_evis = _c[\"evidence\"]\n",
    "                predictions.append((pred_evidences, gold_evis))\n",
    "\n",
    "    for pred, gold in predictions:\n",
    "        if len(pred) == 0:\n",
    "            if len(gold) > 0:\n",
    "                fn += 1\n",
    "        else:\n",
    "            pred_evi = pred[0]\n",
    "            if pred_evi in gold:\n",
    "                tp += 1\n",
    "    \n",
    "    if len(predictions) == 0:\n",
    "        prec_list.append(1)\n",
    "    else:\n",
    "        prec_list.append(tp / len(predictions))\n",
    "        \n",
    "    if tp + fn == 0:\n",
    "        rec_list.append(1)\n",
    "    else:\n",
    "        rec_list.append(tp / (tp + fn))\n",
    "\n",
    "print(prec_list, rec_list)\n",
    "\n",
    "mean_prec = statistics.mean(prec_list)\n",
    "mean_rec = statistics.mean(rec_list)\n",
    "mean_f1 = 2 * mean_rec * mean_prec / (mean_prec + mean_rec)\n",
    "print(\"Precision = {}\".format(mean_prec))\n",
    "print(\"Recall = {}\".format(mean_rec))\n",
    "print(\"F1 = {}\".format(mean_f1))\n",
    "\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
