{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541\n",
      "139\n",
      "227\n",
      "907\n",
      "0.596471885336\n",
      "0.153252480706\n",
      "0.250275633958\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "\n",
    "train_topics = [u'freedom_of_speech', \n",
    "                u'human_rights', u'law',  \n",
    "                u'world_international', \n",
    "                u'economy', u'culture', \n",
    "               ]\n",
    "\n",
    "dev_topics = [  \n",
    "    u'health_and_medicine', u'society',  u'science_and_technology',u'gender', u'education', \n",
    "]\n",
    "\n",
    "test_topics = [\n",
    "    u'politics',\n",
    "    u'digital_freedom', \n",
    "    u'sports_and_entertainments', \n",
    "    u'religion', \n",
    "    u'environment', \n",
    "    u'philosophy', \n",
    "    u'ethics'\n",
    "]\n",
    "\n",
    "split_id = {}\n",
    "\n",
    "total_train_size = 0; \n",
    "total_dev_size = 0; \n",
    "total_test_size = 0; \n",
    "for c in claims:\n",
    "    cId = c['cId']\n",
    "    topics = c['topics']\n",
    "    claim_text = c['text']\n",
    "#     for topic_text in topics:\n",
    "    train_int = len(intersection(topics, train_topics))\n",
    "    dev_int = len(intersection(topics, dev_topics))\n",
    "#     assert train_int == 0 or dev_int == 0 \n",
    "    if train_int > 0: \n",
    "        total_train_size += 1\n",
    "        split_id[cId] = \"train\"\n",
    "    elif dev_int > 0: \n",
    "        total_dev_size += 1\n",
    "        split_id[cId] = \"dev\"\n",
    "    else: \n",
    "        total_test_size += 1\n",
    "        split_id[cId] = \"test\"\n",
    "#         if topic_text in train_topics:\n",
    "#             total_train_size += 1\n",
    "#             break  \n",
    "\n",
    "print(total_train_size)\n",
    "print(total_dev_size)\n",
    "print(total_test_size)\n",
    "print(len(claims))\n",
    "print(total_train_size * 1.0 / len(claims))\n",
    "print(total_dev_size * 1.0 / len(claims))\n",
    "print(total_test_size * 1.0 / len(claims))\n",
    "print((total_train_size + total_test_size + total_dev_size)* 1.0 / len(claims))\n",
    "\n",
    "\n",
    "import json\n",
    "with open('/Users/daniel/ideaProjects/perspective/data/lucene_cach/dataset_split_v0.2.json', 'w') as outfile:\n",
    "    json.dump(split_id, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating clusters of claims that do not share any perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OLD: create stance training data\n",
    "\n",
    "# import json \n",
    "\n",
    "# with open('../data/dataset/perspectrum_with_answers_v0.1.json') as data:\n",
    "#     all_annotations = json.load(data)\n",
    "\n",
    "# with open('../data/dataset/perspective_pool_v0.1.json') as data:\n",
    "#     perspectives = json.load(data)\n",
    "\n",
    "# p_map = {}\n",
    "# for p in perspectives: \n",
    "#     p_map[p['pId']] = p['text']\n",
    "\n",
    "\n",
    "# # create pairs of claims and perspectices and save them in a csv file \n",
    "\n",
    "# import csv\n",
    "\n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "\n",
    "\n",
    "# def save_in_file(claims, file_name): \n",
    "#     with open('../data/dataset/' + file_name, mode='w') as employee_file:\n",
    "#         write = csv.writer(employee_file, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "#         for c in claims: \n",
    "#             claim_text = c[\"text\"]\n",
    "#             for p in c[\"perspectives\"]:                \n",
    "#                 for pid in p[\"pids\"]: \n",
    "#                     if \"SUPPORT\" in p['stance_label_3']: \n",
    "#                         p_text = p_map[pid]\n",
    "#                         print(pid)\n",
    "#                         print(p_text)\n",
    "#                         write.writerow([str(1), str(0), str(0), claim_text, p_text])  \n",
    "#                     if \"UNDERMINE\" in p['stance_label_3']: \n",
    "#                         p_text = p_map[pid]\n",
    "#                         write.writerow([str(0), str(0), str(0), claim_text, p_text])  \n",
    "# split_idx = int(0.7 * len(all_annotations))  \n",
    "        \n",
    "# save_in_file(all_annotations[0:split_idx], 'perspective_stances/train.tsv')\n",
    "# save_in_file(all_annotations[1 + split_idx:], 'perspective_stances/dev.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stance training data\n",
    "\n",
    "import json \n",
    "\n",
    "with open('../data/dataset/perspectrum_with_answers_v0.2.json', encoding='utf-8') as data:\n",
    "    all_annotations = json.load(data)\n",
    "\n",
    "with open('../data/dataset/perspective_pool_v0.2.json', encoding='utf-8') as data:\n",
    "    perspectives = json.load(data)\n",
    "\n",
    "with open('../data/dataset/dataset_split_v0.2.json', encoding='utf-8') as data:\n",
    "    split = json.load(data)\n",
    "    \n",
    "\n",
    "p_map = {}\n",
    "for p in perspectives: \n",
    "    p_map[p['pId']] = p['text']\n",
    "    \n",
    "# create pairs of claims and perspectices and save them in a csv file \n",
    "\n",
    "import csv\n",
    "\n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "\n",
    "def save_in_file(claims, file_name): \n",
    "    with open('../data/dataset/' + file_name, mode='w') as employee_file:\n",
    "        write = csv.writer(employee_file, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for c in claims: \n",
    "            claim_text = c[\"text\"]\n",
    "            for p in c[\"perspectives\"]:                \n",
    "                for pid in p[\"pids\"]: \n",
    "                    if \"SUPPORT\" in p['stance_label_3']: \n",
    "                        p_text = p_map[pid]\n",
    "#                         print(pid)\n",
    "#                         print(p_text)\n",
    "                        write.writerow([str(1), str(0), str(0), claim_text, p_text])  \n",
    "                    if \"UNDERMINE\" in p['stance_label_3']: \n",
    "                        p_text = p_map[pid]\n",
    "                        write.writerow([str(0), str(0), str(0), claim_text, p_text])  \n",
    "\n",
    "train_claims = [_c for _c in all_annotations if split[str(_c['cId'])] == 'train']\n",
    "test_claims = [_c for _c in all_annotations if split[str(_c['cId'])] == 'test']\n",
    "\n",
    "save_in_file(train_claims, 'perspective_stances/train.tsv')\n",
    "save_in_file(test_claims, 'perspective_stances/dev.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 8]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6,8]\n",
    "a[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
