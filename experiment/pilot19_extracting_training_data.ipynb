{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'environment', u'education', u'freedom_of_speech', u'society', u'gender', u'philosophy', u'religion', u'health_and_medicine', u'culture', u'science_and_technology', u'human_rights', u'politics', u'ethics', u'law', u'digital_freedom', u'sports_and_entertainments', u'world_international', u'economy']\n",
      "topic: environment->  44\n",
      "topic: education->  86\n",
      "topic: freedom_of_speech->  57\n",
      "topic: society->  249\n",
      "topic: gender->  44\n",
      "topic: philosophy->  61\n",
      "topic: religion->  36\n",
      "topic: health_and_medicine->  71\n",
      "topic: culture->  146\n",
      "topic: science_and_technology->  77\n",
      "topic: human_rights->  125\n",
      "topic: politics->  285\n",
      "topic: ethics->  110\n",
      "topic: law->  180\n",
      "topic: digital_freedom->  27\n",
      "topic: sports_and_entertainments->  44\n",
      "topic: world_international->  213\n",
      "topic: economy->  124\n"
     ]
    }
   ],
   "source": [
    "# create a hashmap of topicss and use it to split the data test/train/dev \n",
    "\n",
    "import json \n",
    "\n",
    "topics_to_claim = {}\n",
    "\n",
    "def load_json(file_name):\n",
    "    with open(file_name) as data_file:\n",
    "        data = json.loads(data_file.read())\n",
    "        return data\n",
    "\n",
    "claims = load_json('/Users/daniel/ideaProjects/perspective/data/dataset/perspectrum_with_answers_v0.2.json')\n",
    "\n",
    "for c in claims:\n",
    "    topics = c['topics']\n",
    "    claim_text = c['text']\n",
    "    for topic_text in topics:\n",
    "        if topic_text not in topics_to_claim:\n",
    "            topics_to_claim[topic_text] = []\n",
    "        topics_to_claim[topic_text].append(claim_text)\n",
    "\n",
    "print(topics_to_claim.keys())\n",
    "for topic in topics_to_claim.keys(): \n",
    "    print(\"topic: \" + topic + \"->  \"  + str(len(topics_to_claim[topic])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541\n",
      "139\n",
      "227\n",
      "907\n",
      "0.596471885336\n",
      "0.153252480706\n",
      "0.250275633958\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "\n",
    "train_topics = [u'freedom_of_speech', \n",
    "                u'human_rights', u'law',  \n",
    "                u'world_international', \n",
    "                u'economy', u'culture', \n",
    "               ]\n",
    "\n",
    "dev_topics = [  \n",
    "    u'health_and_medicine', u'society',  u'science_and_technology',u'gender', u'education', \n",
    "]\n",
    "\n",
    "test_topics = [\n",
    "    u'politics',\n",
    "    u'digital_freedom', \n",
    "    u'sports_and_entertainments', \n",
    "    u'religion', \n",
    "    u'environment', \n",
    "    u'philosophy', \n",
    "    u'ethics'\n",
    "]\n",
    "\n",
    "split_id = {}\n",
    "\n",
    "total_train_size = 0; \n",
    "total_dev_size = 0; \n",
    "total_test_size = 0; \n",
    "for c in claims:\n",
    "    cId = c['cId']\n",
    "    topics = c['topics']\n",
    "    claim_text = c['text']\n",
    "#     for topic_text in topics:\n",
    "    train_int = len(intersection(topics, train_topics))\n",
    "    dev_int = len(intersection(topics, dev_topics))\n",
    "#     assert train_int == 0 or dev_int == 0 \n",
    "    if train_int > 0: \n",
    "        total_train_size += 1\n",
    "        split_id[cId] = \"train\"\n",
    "    elif dev_int > 0: \n",
    "        total_dev_size += 1\n",
    "        split_id[cId] = \"dev\"\n",
    "    else: \n",
    "        total_test_size += 1\n",
    "        split_id[cId] = \"test\"\n",
    "#         if topic_text in train_topics:\n",
    "#             total_train_size += 1\n",
    "#             break  \n",
    "\n",
    "print(total_train_size)\n",
    "print(total_dev_size)\n",
    "print(total_test_size)\n",
    "print(len(claims))\n",
    "print(total_train_size * 1.0 / len(claims))\n",
    "print(total_dev_size * 1.0 / len(claims))\n",
    "print(total_test_size * 1.0 / len(claims))\n",
    "print((total_train_size + total_test_size + total_dev_size)* 1.0 / len(claims))\n",
    "\n",
    "\n",
    "import json\n",
    "with open('/Users/daniel/ideaProjects/perspective/data/lucene_cach/dataset_split_v0.2.json', 'w') as outfile:\n",
    "    json.dump(split_id, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating clusters of claims that do not share any perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracting stance training data\n",
    "\n",
    "import json \n",
    "\n",
    "with open('../data/dataset/perspectrum_with_answers_v0.1.json') as data:\n",
    "  all_annotations = json.load(data)\n",
    "\n",
    "with open('../data/dataset/perspective_pool_v0.1.json') as data:\n",
    "  perspectives = json.load(data)\n",
    "\n",
    "p_map = {}\n",
    "for p in perspectives: \n",
    "    p_map[p['pId']] = p['text']\n",
    "\n",
    "\n",
    "# create pairs of claims and perspectices and save them in a csv file \n",
    "\n",
    "import csv\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "\n",
    "def save_in_file(claims, file_name): \n",
    "    with open('../data/dataset/' + file_name, mode='w') as employee_file:\n",
    "        write = csv.writer(employee_file, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for c in claims: \n",
    "            claim_text = c[\"text\"]\n",
    "            for p in c[\"perspectives\"]:                \n",
    "                for pid in p[\"pids\"]: \n",
    "                    if \"SUPPORT\" in p['stance_label_3']: \n",
    "                        p_text = p_map[pid]\n",
    "                        print(pid)\n",
    "                        print(p_text)\n",
    "                        write.writerow([str(1), str(0), str(0), claim_text, p_text])  \n",
    "                    if \"UNDERMINE\" in p['stance_label_3']: \n",
    "                        p_text = p_map[pid]\n",
    "                        write.writerow([str(0), str(0), str(0), claim_text, p_text])  \n",
    "split_idx = int(0.7 * len(all_annotations))  \n",
    "        \n",
    "save_in_file(all_annotations[0:split_idx], 'perspective_stances/train.tsv')\n",
    "save_in_file(all_annotations[1 + split_idx:], 'perspective_stances/dev.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 8]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6,8]\n",
    "a[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
