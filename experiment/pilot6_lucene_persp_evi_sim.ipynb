{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8122\n",
      "4645\n",
      "4645\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import query_elasticsearch as es\n",
    "\n",
    "# This is the gold training data from pilot 3 or 4\n",
    "evidence_path = \"/home/squirrel/ccg-new/projects/perspective/data/pilot3_twowingos/110718-training-data/evidence.json\"\n",
    "claim_persp_path = \"/home/squirrel/ccg-new/projects/perspective/data/pilot3_twowingos/110718-training-data/claim_perspective.json\"\n",
    "annotation_path = \"/home/squirrel/ccg-new/projects/perspective/data/pilot3_twowingos/110718-training-data/gold_annotation.json\"\n",
    "\n",
    "with open(evidence_path, 'r', encoding='utf-8') as fin:\n",
    "    evidences = json.load(fin)\n",
    "\n",
    "with open(claim_persp_path, 'r', encoding='utf-8') as fin:\n",
    "    persps = json.load(fin)\n",
    "    \n",
    "with open(annotation_path, 'r', encoding='utf-8') as fin:\n",
    "    annotations = json.load(fin)\n",
    "\n",
    "print(len(evidences))\n",
    "print(len(persps))\n",
    "print(len(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4645 entries, 0 to 4644\n",
      "Data columns (total 2 columns):\n",
      "evidence_id       4645 non-null int64\n",
      "perspective_id    4645 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 72.7 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adf = pd.DataFrame(annotations)\n",
    "adf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each perspective generate evidence candidates\n",
    "results = []\n",
    "for p in persps:\n",
    "    # top 50 candidates\n",
    "    _cands = es.get_top_evidences(p['title'])\n",
    "    cands = [(eid, score) for e, eid, score in _cands]\n",
    "    results.append({\n",
    "        'persp_id': p['id'],\n",
    "        'candidates': cands\n",
    "    })\n",
    "\n",
    "# save results\n",
    "out_path = \"/home/squirrel/ccg-new/projects/perspective/data/pilot6_lucene/pp_2_ev.json\"\n",
    "\n",
    "with open(out_path, 'w') as fout:\n",
    "    json.dump(results, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy: 0.36404736275565125\n",
      "Top 10 accuracy: 0.7791173304628632\n",
      "Top 25 accuracy: 0.8716899892357374\n",
      "Top 50 accuracy: 0.9121636167922498\n"
     ]
    }
   ],
   "source": [
    "# For each perspective, measure top k accuracy of evidence candidates\n",
    "\n",
    "path = \"/home/squirrel/ccg-new/projects/perspective/data/pilot6_lucene/pp_2_ev.json\"\n",
    "\n",
    "with open(path) as fin:\n",
    "    results = json.load(fin)\n",
    "\n",
    "def get_gold_evidence(perspective_id, adf):\n",
    "    q = adf.loc[adf.perspective_id == perspective_id]\n",
    "    if len(q) > 0:\n",
    "        return q.iloc[0].evidence_id\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "k = [1, 10, 25, 50]\n",
    "correct = [0 for _k in k]\n",
    "\n",
    "for res in results:\n",
    "    gold = get_gold_evidence(res['persp_id'], adf)\n",
    "    for idx, _k in enumerate(k):\n",
    "        cands = [eid for eid, score in res['candidates']]\n",
    "        \n",
    "        if gold in cands[:_k]:\n",
    "            correct[idx] += 1\n",
    "\n",
    "total = len(results)\n",
    "\n",
    "\n",
    "for idx, _k in enumerate(k):\n",
    "    print(\"Top {} accuracy: {}\".format(_k, correct[idx] / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The other way around: for each evidence, generate perspective candidates\n",
    "from elasticsearch.exceptions import RequestError\n",
    "\n",
    "results = []\n",
    "for e in evidences:\n",
    "    # top 50 candidates\n",
    "    try:\n",
    "        _cands = es.get_top_perspectives(e['content'])\n",
    "    except RequestError:\n",
    "        _cands = []\n",
    "    cands = [(pid, score) for p, pid, score in _cands]\n",
    "    results.append({\n",
    "        'evidence_id': e['id'],\n",
    "        'candidates': cands\n",
    "    })\n",
    "\n",
    "# save results\n",
    "out_path = \"/home/squirrel/ccg-new/projects/perspective/data/pilot6_lucene/ev_2_pp.json\"\n",
    "\n",
    "with open(out_path, 'w') as fout:\n",
    "    json.dump(results, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy: 0.38256189451022604\n",
      "Top 10 accuracy: 0.7089343379978471\n",
      "Top 25 accuracy: 0.7885898815931108\n",
      "Top 50 accuracy: 0.8320775026910656\n"
     ]
    }
   ],
   "source": [
    "# For each evidence, measure top k accuracy of perspective candidates\n",
    "\n",
    "path = \"/home/squirrel/ccg-new/projects/perspective/data/pilot6_lucene_persp_evidence/ev_2_pp.json\"\n",
    "\n",
    "with open(path) as fin:\n",
    "    results = json.load(fin)\n",
    "    \n",
    "def get_gold_perspective(evidence_id, adf):\n",
    "    q = adf.loc[adf.evidence_id == evidence_id]\n",
    "    if len(q) > 0:\n",
    "        return q.iloc[0].perspective_id\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "k = [1, 10, 25, 50]\n",
    "correct = [0 for _k in k]\n",
    "total = 0\n",
    "\n",
    "for res in results:\n",
    "    gold = get_gold_perspective(res['evidence_id'], adf)\n",
    "    \n",
    "    if gold == None:\n",
    "        continue\n",
    "        \n",
    "    total += 1\n",
    "    for idx, _k in enumerate(k):\n",
    "        if len(res['candidates']) > 0:\n",
    "            cands = [pid for pid, score in res['candidates']]\n",
    "            if gold in cands[:_k]:\n",
    "                correct[idx] += 1\n",
    "\n",
    "for idx, _k in enumerate(k):\n",
    "    print(\"Top {} accuracy: {}\".format(_k, correct[idx] / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
